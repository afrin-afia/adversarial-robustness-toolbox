# Instructions for manual replication of this experiment

In this experiment, we observe the effects of adversarial attacks on text dataset. Required source codes are available inside the `src` folder. 

* To generate adversarial attacks on text data, execute the `src/fgsm_pgd_text_data.py` file.
* To observe how the `model hardening` defense mechanism works, execute the `src/model_hardening_text_data.py` file.
* Rest two python files produce graphs and plots.

The directory `sgird/in` contains the input file. 
The directory `sgrid/outs` contains the output files.
